"""
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
以下部分是Text2Tex的视角转换的texturing贴图生成的代码
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

"""




# common utils
import os
import argparse
import time
import imageio
import pdb
from tqdm import tqdm
import cv2
import pickle
import json
from scipy import stats

# pytorch3d
from pytorch3d.renderer import TexturesUV
from pytorch3d.renderer.mesh.shader import ShaderBase
from pytorch3d.renderer import (
    AmbientLights,
    SoftPhongShader
)
from pytorch3d.ops import interpolate_face_attributes
import sys
sys.path.append(".")


from lib.render_helper import init_renderer, render
from lib.projection_helper import (build_backproject_mask, build_diffusion_mask, compose_quad_mask, compute_view_heat)
from lib.shading_helper import init_soft_phong_shader

from lib.camera_helper import init_camera
from lib.render_helper import init_renderer, render
from lib.shading_helper import (
    BlendParams,
    init_soft_phong_shader,
    init_flat_texel_shader,
)
from lib.vis_helper import visualize_outputs, visualize_quad_mask
from lib.constants import *


# torch
import torch

from torchvision import transforms

# numpy
import numpy as np

# image
from PIL import Image

from pytorch3d.renderer import (
    PerspectiveCameras,
    look_at_view_transform,
    FoVPerspectiveCameras
)

# customized
import sys
sys.path.append(".")

from lib.mesh_helper import (
    init_mesh_2,
    apply_offsets_to_mesh,
    adjust_uv_map
)
from lib.render_helper import render
from lib.io_helper import (
    save_backproject_obj,
    save_args,
    save_viewpoints
)
from lib.vis_helper import (
    visualize_outputs, 
    visualize_principle_viewpoints, 
    visualize_refinement_viewpoints
)
from lib.diffusion_helper import (
    get_controlnet_depth,
    get_inpainting,
    apply_controlnet_depth,
    apply_inpainting_postprocess
)
from lib.projection_helper import (
    get_all_4_locations,
    select_viewpoint,
)
from lib.camera_helper import init_viewpoints


# Setup
if torch.cuda.is_available():
    DEVICE = torch.device("cuda:2")
    torch.cuda.set_device(DEVICE)
else:
    print("no gpu avaiable")
    exit()


"""
    Use Diffusion Models conditioned on depth input to back-project textures on 3D mesh.
    在3D网格上使用基于深度输入的扩散模型来反向投影纹理。
    The inputs should be constructed as follows(输入应构造如下):
        - <input_dir>/
            |- <obj_file> # name of the input OBJ file
    
    此脚本的输出将存储在' outputs/ '下，并使用配置参数作为文件夹名称。具体来说，应该有以下文件在这样的文件夹:
        - outputs/
            |- <configs>/                       # configurations of the run
                |- generate/                    # assets generated in generation stage
                    |- depth/                   # depth map
                    |- inpainted/               # images generated by diffusion models
                    |- intermediate/            # renderings of textured mesh after each step
                    |- mask/                    # generation mask
                    |- mesh/                    # textured mesh
                    |- normal/                  # normal map
                    |- rendering/               # input renderings
                    |- similarity/              # simiarity map
                |- update/                      # assets generated in refinement stage
                    |- ...                      # the structure is the same as generate/
                |- args.json                    # all arguments for the run
                |- viewpoints.json              # all viewpoints
                |- principle_viewpoints.png     # principle viewpoints
                |- refinement_viewpoints.png    # refinement viewpoints

"""

def init_args():
    print("=> initializing input arguments...")
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_dir", type=str)
    parser.add_argument("--obj_name", type=str)
    parser.add_argument("--obj_file", type=str)
    parser.add_argument("--prompt", type=str)
    parser.add_argument("--a_prompt", type=str, default="best quality, high quality, extremely detailed, good geometry")
    parser.add_argument("--n_prompt", type=str, default="deformed, extra digit, fewer digits, cropped, worst quality, low quality, smoke")
    parser.add_argument("--new_strength", type=float, default=1)
    parser.add_argument("--update_strength", type=float, default=0.5)
    parser.add_argument("--ddim_steps", type=int, default=20)
    parser.add_argument("--guidance_scale", type=float, default=10)
    parser.add_argument("--output_scale", type=float, default=1)
    parser.add_argument("--view_threshold", type=float, default=0.1)
    parser.add_argument("--num_viewpoints", type=int, default=8)
    parser.add_argument("--viewpoint_mode", type=str, default="predefined", choices=["predefined", "hemisphere"])
    parser.add_argument("--update_steps", type=int, default=8)
    parser.add_argument("--update_mode", type=str, default="heuristic", choices=["sequential", "heuristic", "random"])
    parser.add_argument("--blend", type=float, default=0.5)
    parser.add_argument("--eta", type=float, default=0.0)
    parser.add_argument("--seed", type=int, default=42)

    # parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--cuda", type=int, default=0)
    parser.add_argument("--work_dir", type=str, required=True)
    parser.add_argument("--sample", type=str, required=True)
    parser.add_argument("--sample_dir", type=str, required=True)
    parser.add_argument("--img_size", type=int, default=1024)
    parser.add_argument("--category", type=str, required=True)



    parser.add_argument("--use_patch", action="store_true", help="apply repaint during refinement to patch up the missing regions")
    parser.add_argument("--use_multiple_objects", action="store_true", help="operate on multiple objects")
    parser.add_argument("--use_principle", action="store_true", help="poperate on multiple objects")
    parser.add_argument("--use_shapenet", action="store_true", help="operate on ShapeNet objects")
    parser.add_argument("--use_objaverse", action="store_true", help="operate on Objaverse objects")
    parser.add_argument("--use_unnormalized", action="store_true", help="save unnormalized mesh")

    parser.add_argument("--add_view_to_prompt", action="store_true", help="add view information to the prompt")
    parser.add_argument("--post_process", action="store_true", help="post processing the texture")

    parser.add_argument("--smooth_mask", action="store_true", help="smooth the diffusion mask")

    parser.add_argument("--force", action="store_true", help="forcefully generate more image")

    # negative options
    parser.add_argument("--no_repaint", action="store_true", help="do NOT apply repaint")
    parser.add_argument("--no_update", action="store_true", help="do NOT apply update")

    # device parameters
    # parser.add_argument("--device", type=str, choices=["a6000", "2080"], default="a6000")
    parser.add_argument("--device", type=str, choices=["A40", "2080"], default="a6000")
    # camera parameters NOTE need careful tuning!!!
    parser.add_argument("--test_camera", action="store_true")
    parser.add_argument("--dist", type=float, default=1, 
        help="distance to the camera from the object")
    parser.add_argument("--elev", type=float, default=0,
        help="the angle between the vector from the object to the camera and the horizontal plane")
    parser.add_argument("--azim", type=float, default=180,
        help="the angle between the vector from the object to the camera and the vertical plane")

    args = parser.parse_args()

    if args.device == "a6000":  #后续正式运行时应该改为"A40"
        setattr(args, "render_simple_factor", 12) #图像渲染的简化因子。这个值通常用于控制渲染过程中的精细程度。较大的值可以提高图像的精细度
        setattr(args, "fragment_k", 1)
        setattr(args, "image_size", 768)
        setattr(args, "uv_size", 3000)
    else:
        setattr(args, "render_simple_factor", 4)
        setattr(args, "fragment_k", 1)
        setattr(args, "image_size", 768)
        setattr(args, "uv_size", 1000)

    return args


#############################################
def P2B(R:np.ndarray, T:np.ndarray)->np.ndarray:
    P2B_R1 = np.array([[1,0,0],[0,0,-1],[0,1,0]], dtype=np.float64)
    P2B_R2 = np.array([[-1,0,0],[0,1,0],[0,0,-1]], dtype=np.float64)
    P2B_T  = np.array([[-1,0,0],[0,0,1],[0,-1,0]], dtype=np.float64)
    vec4w  = np.array([[0,0,0,1]], dtype=np.float64)
    Bcol3 = P2B_T @ R @ T
    B3x3  = P2B_R1 @ R @ P2B_R2
    B3x4 = np.concatenate([B3x3, Bcol3[:,None]], axis=1)
    B = np.concatenate([B3x4,vec4w], axis=0)
    return B

# blender to pytorch3d
def B2P(B):
    B2P_R1 = np.array([[1,0,0],[0,0,1],[0,-1,0]], dtype=np.float64)
    B2P_R2 = np.array([[-1,0,0],[0,1,0],[0,0,-1]], dtype=np.float64)
    B2P_T  = np.array([[-1,0,0],[0,0,-1],[0,1,0]], dtype=np.float64)
    R = B2P_R1 @ B[:3, :3] @ B2P_R2
    T = B2P_T @ B[:3, 3] @ R

    return R, T

def to_rgb(label, palette):
    h,w = label.shape
    rgb = np.zeros((h, w, 3), dtype=np.uint8)
    for i in range(h):
        for j in range(w):
            value = label[i,j]
            rgb_value = palette[value]
            rgb[i,j] = rgb_value

    return rgb

def resize_without_interpolation(input_image, new_size):
    old_size = input_image.shape
    row_ratio, col_ratio = old_size[0]/new_size[0], old_size[1]/new_size[1]

    output_image = np.zeros(new_size)

    for i in range(new_size[0]):
        for j in range(new_size[1]):
            output_image[i, j] = input_image[int(i*row_ratio), int(j*col_ratio)]

    return output_image.astype(np.uint8)

def solve(matrix):
    # 找到矩阵中的连通域

    kernel = np.ones((2, 2), np.uint8)

    img = cv2.erode(matrix, kernel, iterations=1)

    num_labels, labels = cv2.connectedComponents((img!=0).astype(np.uint8))
    # cv2.imwrite('original.png', matrix*20)
    # cv2.imwrite('erode.png', img * 20)

    # 对于每个连通域，找出出现次数最多的元素，然后将该连通域的元素统一为出现次数最多的元素
    # print(num_labels)

    for label in range(1, num_labels):
        # print(label)
        mask = (labels == label)

        if np.any(mask):  # 如果有任何元素满足条件
            most_frequent = stats.mode(matrix[mask])[0]
            matrix[mask] = most_frequent

    return matrix


def init_camera(image_size, device,B_idx):
    
    R, T = B2P(B_idx)
    R = torch.tensor(R).unsqueeze(0)
    T = _tensor = torch.tensor(T).unsqueeze(0)  #(3,) 转换为 PyTorch 张量，并添加一个维度
    # B = P2B(R.cpu().numpy()[0],T.cpu().numpy()[0])
    image_size = torch.tensor([image_size, image_size]).unsqueeze(0)
    cameras = PerspectiveCameras(R=R, T=T, device=device, image_size=image_size)
    
    
    
    return cameras


def render_one_view(mesh,
    B_idx, 
    image_size, faces_per_pixel,
    device):

    # render the view
    cameras = init_camera(
        image_size, device, B_idx
    )
    renderer = init_renderer(cameras,
        shader=init_soft_phong_shader(
            camera=cameras,
            blend_params=BlendParams(),
            device=device),
        image_size=image_size, 
        faces_per_pixel=faces_per_pixel
    )

    init_images_tensor, normal_maps_tensor, similarity_tensor, depth_maps_tensor, fragments = render(mesh, renderer)
    
    return (
        cameras, renderer,
        init_images_tensor, normal_maps_tensor, similarity_tensor, depth_maps_tensor, fragments
    )

def render_one_view_and_build_masks(B_idx, 
    selected_view_idx,
    similarity_texture_cache, exist_material,
    mesh, faces, verts_uvs,
    image_size, faces_per_pixel,
    device, smooth_mask=False, view_threshold=0.01):
    
    
    #pdb.set_trace()
    # render the view
    (
        cameras, renderer,
        init_images_tensor, normal_maps_tensor, similarity_tensor, depth_maps_tensor, fragments
    ) = render_one_view(mesh,
        B_idx,
        image_size, faces_per_pixel,
        device
    )
    
    init_image = init_images_tensor[0].cpu()
    init_image = init_image.permute(2, 0, 1)
    init_image = transforms.ToPILImage()(init_image).convert("RGB")

    

    normal_map = normal_maps_tensor[0].cpu()
    normal_map = normal_map.permute(2, 0, 1)
    normal_map = transforms.ToPILImage()(normal_map).convert("RGB")

    depth_map = depth_maps_tensor[0].cpu().numpy()
    depth_map = Image.fromarray(depth_map).convert("L")

    similarity_map = similarity_tensor[0, :, :, 0].cpu()
    similarity_map = transforms.ToPILImage()(similarity_map).convert("L")


    flat_renderer = init_renderer(cameras,
        shader=init_flat_texel_shader(
            camera=cameras,
            device=device),
        image_size=image_size,
        faces_per_pixel=faces_per_pixel
    )
    new_mask_image, update_mask_image, old_mask_image, exist_mask_image = build_diffusion_mask(
        (mesh, faces, verts_uvs), 
        flat_renderer, exist_material, similarity_texture_cache, selected_view_idx, device, image_size, 
        smooth_mask=smooth_mask, view_threshold=view_threshold
    )
    # NOTE the view idx is the absolute idx in the sample space (i.e. `selected_view_idx`)
    # it should match with `similarity_texture_cache`

    (
        old_mask_tensor, 
        update_mask_tensor, 
        new_mask_tensor, 
        all_mask_tensor, 
        quad_mask_tensor
    ) = compose_quad_mask(new_mask_image, update_mask_image, old_mask_image, device)

    view_heat = compute_view_heat(similarity_tensor, quad_mask_tensor)
#     view_heat *= view_punishments[selected_view_idx]

    # save intermediate results


    return (
        view_heat,
        renderer, cameras, fragments,
        init_image, normal_map, depth_map, 
        init_images_tensor, normal_maps_tensor, depth_maps_tensor, similarity_tensor, 
        old_mask_image, update_mask_image, new_mask_image, 
        old_mask_tensor, update_mask_tensor, new_mask_tensor, all_mask_tensor, quad_mask_tensor
    )


def build_similarity_texture_cache_for_all_views(mesh, faces, verts_uvs,
    B, view_num,
    image_size, image_size_scaled, uv_size, faces_per_pixel,
    device):

    num_candidate_views = view_num
    similarity_texture_cache = torch.zeros(num_candidate_views, uv_size, uv_size).to(device)

    print("=> building similarity texture cache for all views...")
    for i in tqdm(range(num_candidate_views)):

        cameras, _, _, _, similarity_tensor, _, _ = render_one_view(mesh,
            B[i],
            image_size, faces_per_pixel, device)

        similarity_texture_cache[i] = build_backproject_mask(mesh, faces, verts_uvs, 
            cameras, transforms.ToPILImage()(similarity_tensor[0, :, :, 0]).convert("RGB"), faces_per_pixel,
            image_size_scaled, uv_size, device)

    return similarity_texture_cache


def backproject_from_image(mesh, faces, verts_uvs, cameras,
                           reference_image, new_mask_image, update_mask_image,
                           init_texture, exist_texture, init_material_stack,
                           image_size, uv_size, faces_per_pixel,
                           device):
    # construct pixel UVs
    renderer_scaled = init_renderer(cameras,
                                    shader=init_soft_phong_shader(
                                        camera=cameras,
                                        blend_params=BlendParams(),
                                        device=device),
                                    image_size=image_size,
                                    faces_per_pixel=faces_per_pixel
                                    )
    fragments_scaled = renderer_scaled.rasterizer(mesh)

    # get UV coordinates for each pixel
    faces_verts_uvs = verts_uvs[faces.textures_idx]

    pixel_uvs = interpolate_face_attributes(
        fragments_scaled.pix_to_face, fragments_scaled.bary_coords, faces_verts_uvs
    )  # NxHsxWsxKx2
    pixel_uvs = pixel_uvs.permute(0, 3, 1, 2, 4).reshape(pixel_uvs.shape[-2], pixel_uvs.shape[1], pixel_uvs.shape[2], 2)

    # the update mask has to be on top of the diffusion mask
    new_mask_image_tensor = transforms.ToTensor()(new_mask_image).to(device).unsqueeze(-1)
    update_mask_image_tensor = transforms.ToTensor()(update_mask_image).to(device).unsqueeze(-1)

    project_mask_image_tensor = torch.logical_or(update_mask_image_tensor, new_mask_image_tensor).float()
    project_mask_image = project_mask_image_tensor * 255.
    project_mask_image = Image.fromarray(project_mask_image[0, :, :, 0].cpu().numpy().astype(np.uint8))

    project_mask_image_scaled = project_mask_image.resize(
        (image_size, image_size),
        Image.Resampling.NEAREST
    )
    project_mask_image_tensor_scaled = transforms.ToTensor()(project_mask_image_scaled).to(device)



    pixel_uvs_masked = pixel_uvs[project_mask_image_tensor_scaled == 1]

    texture_locations_y, texture_locations_x = get_all_4_locations(
        (1 - pixel_uvs_masked[:, 1]).reshape(-1) * (uv_size - 1),
        pixel_uvs_masked[:, 0].reshape(-1) * (uv_size - 1)
    )

    K = pixel_uvs.shape[0]
    project_mask_image_tensor_scaled = project_mask_image_tensor_scaled[:, None, :, :, None].repeat(1, 4, 1, 1, 3)
    # texture_values = torch.from_numpy(np.array(reference_image.resize((image_size, image_size))))
    #########此处增加一个对于单通道材质图的判断和处理
    # 将reference_image转换为NumPy数组
    reference_image_np = np.array(reference_image)
    # 使用Pillow的Image.fromarray来创建图像
    reference_image_pil = Image.fromarray(reference_image_np)
    reference_image_pil = reference_image_pil.convert('L')
    if reference_image_pil.mode == "L":
        # 如果 reference_image 是单通道灰度图像
        reference_image_pil = reference_image_pil.convert("RGB")  # 将其转换为三通道的彩色图像

    resized_image = reference_image_pil.resize((image_size, image_size), Image.LANCZOS)

    # 现在 `resized_image` 应该是三通道的彩色图像
    texture_values = torch.from_numpy(np.array(resized_image))

    texture_values = texture_values.to(device).unsqueeze(0).expand([4, -1, -1, -1]).unsqueeze(0).expand(
        [K, -1, -1, -1, -1])

    texture_values_masked = texture_values.reshape(-1, 3)[project_mask_image_tensor_scaled.reshape(-1, 3) == 1].reshape(
        -1, 3)

    # texture


    temp = torch.from_numpy(np.ones((uv_size, uv_size, 3)).astype(np.uint8) * 255).to(device)

    temp[texture_locations_y, texture_locations_x, :] = texture_values_masked
    temp_ = temp[:, :, :1]

    new_stack = np.concatenate((init_material_stack, np.array(temp_.cpu())), axis=2)

    texture_tensor = torch.from_numpy(np.array(init_texture)).to(device)
    texture_tensor[texture_locations_y, texture_locations_x, :] = texture_values_masked



    init_texture = Image.fromarray(texture_tensor.cpu().numpy().astype(np.uint8))

    # update texture cache
    exist_texture[texture_locations_y, texture_locations_x] = 1

    return init_texture, project_mask_image, exist_texture, new_stack



if __name__ == "__main__":
    args = init_args()

    label_dict = {'car':np.array([1,2,3,4,5]),
                'furniture':np.array([1, 2, 5, 6, 7, 8]),
                'instrument':np.array([1,2,5,6,8]),
                'building':np.array([2,4,6,9,10,11]),
                'plant':np.array([6,11,12,13,14])}
    # furniture
    # label_range = np.array([1, 2, 5, 6, 7, 8])

    # car
    label_range = label_dict[args.category]

    # instrument
    # label_range = np.array([1,2,5,6,8])

    # building
    # label_range = np.array([2,4,6,9,10,11])

    # plant
    # label_range = np.array([6,11,12,13,14])

    palette = [[0, 0, 0], [244, 35, 232], [70, 70, 70], [102, 102, 156],
               [190, 153, 153], [153, 153, 153], [250, 170, 30], [220, 220, 0],
               [107, 142, 35], [152, 251, 152], [70, 130, 180],
               [220, 20, 60], [255, 0, 0], [0, 0, 142], [0, 0, 70],
               [0, 60, 100], [0, 80, 100], [0, 0, 230], [119, 11, 32], [100, 100, 0]]
    
    # output_dir = 'output'
    # output_dir = args.output_dir
    view_num = 41
    # DEVICE = torch.device("cuda:2")
    DEVICE = torch.device(f"cuda:{args.cuda}")
    torch.cuda.set_device(DEVICE)
    B = np.load("/path-to-MaterialSeg3D/Text2Tex/camera_angle/original_41.npy")
    
    # work_dir = "/path-to-MaterialSeg3D/output/predict_mapping/"
    work_dir = args.work_dir
    sample = args.sample


    # if sample in ['car2']:
    #     continue
    # if sample in os.listdir("/home/ruitong_gan/3D/Text2Tex/work_dir/add/plant/UV/"):
    #     continue
    print('now the sample is ', sample)
    # input_dir = os.path.join("/path-to-MaterialSeg3D/example/" , sample)
    input_dir = args.sample_dir
    obj_file = sample + '.obj'
    inference_dir = os.path.join(work_dir, sample)
    UV_dir = inference_dir.replace('predict_mapping', 'UV')
    image_list = os.listdir(inference_dir)
    # assert len(image_list) == view_num



    uv_size = 3000
    image_size = args.img_size


    # if args.no_repaint: output_dir += "-norepaint"
    # if args.no_update: output_dir += "-noupdate"
    #
    # os.makedirs(output_dir, exist_ok=True)
    # print("=> OUTPUT_DIR:", output_dir)

    #初始化各类用于生成和更新3D模型纹理的资源
    # init mesh初始化3D物体的网格，通过加载指定路径下的OBJ文件来创建模型的三角网格
    # pdb.set_trace()
    mesh, _, faces, aux, principle_directions, mesh_center, mesh_scale = init_mesh_2(os.path.join(input_dir, obj_file), DEVICE)

    # gradient texture打开一个空白的假纹理图像dummy.png，用于初始化纹理

    init_material= Image.open("./samples/textures/dummy.png").convert("RGB").resize((uv_size, uv_size))
    init_material_stack = np.ones((uv_size, uv_size, 1)) * 255
    # HACK adjust UVs for multiple materials
    # pdb.set_trace()
    new_verts_uvs = aux.verts_uvs    #如果只是单个物体，则保留原始的UV映射

    # update the mesh 在更新过程中，将模型的顶点信息与 UV 图上的信息进行对照，以确保纹理图像————>3D模型mesh表面。
    mesh.textures = TexturesUV(
        maps=transforms.ToTensor()(init_material)[None, ...].permute(0, 2, 3, 1).to(DEVICE), #maps 参数接受了一个纹理图像，这个图像会被映射到模型的表面上。这个图像经过了处理，以确保它的格式适用于 PyTorch 3D 库。
        faces_uvs=faces.textures_idx[None, ...],  #faces_uvs 参数包含了三角面片的 UV 坐标索引。每个三角面片都有一个对应的 UV 坐标索引，用于确定在 UV 图上的哪个位置应用纹理。
        verts_uvs=new_verts_uvs[None, ...]  #verts_uvs 参数包含了模型的顶点的 UV 坐标。每个顶点都有一个 UV 坐标，用于确定在 UV 图上的哪个位置应用纹理
    )


    # back-projected faces
    #创建了一个全零的二维张量，用于保存控制深度到图像的投影。它是一个用于深度到图像映射的辅助张量
    exist_material = torch.from_numpy(np.zeros([uv_size, uv_size]).astype(np.float32)).to(DEVICE)

    # initialize viewpoints 初始化了视点（观察角度）。它包括主要的用于生成的原始视点（principle viewpoints），以及用于更新的细化视点（refinement viewpoints）
    # including: principle viewpoints for generation + refinement viewpoints for updating


    # save args将初始化参数保存到指定的输出目录中，以便将来参考和记录
    # save_args(args, output_dir)

    # initialize depth2image model 初始化了用于控制深度到图像的投影的模型，以及一个采样器。这将在模型生成和更新过程中用于深度到图像的计算
    controlnet, ddim_sampler = get_controlnet_depth()


    # ------------------- 下面开始为操作的代码区 ------------------------

    # 1. generate texture with RePaint 生成纹理
    # NOTE no update / refinement

    # material_dir = os.path.join(output_dir, "material")
    # os.makedirs(material_dir, exist_ok=True)




    pre_similarity_texture_cache = build_similarity_texture_cache_for_all_views(mesh, faces, new_verts_uvs,
        B, view_num,
        image_size, image_size * 12, uv_size, 1,
        DEVICE
    )

    ###############开始生成纹理的过程：生成图像 + 创建纹理UV ########################
    # start generation
    print("=> start generating texture...")
    start_time = time.time()
    for view_idx in range(view_num):  #对每个视点进行处理\
        exist_material = torch.from_numpy(np.zeros([uv_size, uv_size]).astype(np.float32)).to(DEVICE)

        print("=> processing view {}...".format(view_idx))

        # sequentially pop the viewpoints
        # 通过 pre_dist_list, pre_elev_list, pre_azim_list, pre_sector_list获取当前视点的位置信息


        # 1.1. render and build masks（生成部分的核心代码）
        #render_one_view_and_build_masks: 这个函数用于渲染一个视点并构建蒙版。它接受一系列参数，包括距离（dist）、仰角（elev）、方位角（azim）等，用来定义视点的位置和特性。
        #然后，它使用这些参数渲染视点，生成图像、法线图、深度图等。这个函数还会构建多个蒙版，包括保留蒙版（keep_mask_image）、更新蒙版（update_mask_image）、生成蒙版（generate_mask_image）等

        (
            view_score,
            renderer, cameras, fragments,
            init_image, normal_map, depth_map,
            init_images_tensor, normal_maps_tensor, depth_maps_tensor, similarity_tensor,
            keep_mask_image, update_mask_image, generate_mask_image,
            keep_mask_tensor, update_mask_tensor, generate_mask_tensor, all_mask_tensor, quad_mask_tensor,
        ) = render_one_view_and_build_masks(B[view_idx],
            view_idx,# => actual view idx and the sequence idx
            pre_similarity_texture_cache, exist_material,
            mesh, faces, new_verts_uvs,
            image_size, 1,
            DEVICE, smooth_mask=args.smooth_mask, view_threshold=args.view_threshold
        )

        # 1.2. generate missing region生成缺失区域的纹理
        # NOTE first view still gets the mask for consistent ablations
        if args.no_repaint and view_idx != 0: #如果 args.no_repaint 为 True 并且 view_idx 不等于 0，也就是对于后续的视点（非第一个视点），则创建一个全白的图像作为 actual_generate_mask_image。
            actual_generate_mask_image = Image.fromarray((np.ones_like(np.array(generate_mask_image)) * 255.).astype(np.uint8))
        else:
            actual_generate_mask_image = generate_mask_image  #对于第一个视点或者重新绘制的视点，将 generate_mask_image 赋值给 actual_generate_mask_image，会根据生成蒙版重新绘制图像

        print("=> generate for view {}".format(view_idx))

        #@@@@@@@@@！！！！！！使用 apply_controlnet_depth 函数，根据输入参数，以及视点的图像、初始掩模、深度地图等信息，生成缺失区域的纹理########
        # generate_image, generate_image_before, generate_image_after = apply_controlnet_depth(controlnet, ddim_sampler,
        #     init_image.convert("RGBA"), prompt, args.new_strength, args.ddim_steps,
        #     actual_generate_mask_image, keep_mask_image, depth_maps_tensor.permute(1, 2, 0).repeat(1, 1, 3).cpu().numpy(),
        #     args.a_prompt, args.n_prompt, args.guidance_scale, args.seed, args.eta, 1, DEVICE, args.blend)
        #生成前、生成后、和生成的差异图像
#         generate_image.save(os.path.join(inpainted_image_dir, "{}.png".format(view_idx)))
#         generate_image_before.save(os.path.join(inpainted_image_dir, "{}_before.png".format(view_idx)))
#         generate_image_after.save(os.path.join(inpainted_image_dir, "{}_after.png".format(view_idx)))

        # 1.2.2 back-project and create texture
        # NOTE projection mask = generate mask

        ##@@@！！！！！！！！！！！使用 backproject_from_image 函数，将生成的图像重新投影到 3D 模型上，同时更新初始纹理。
        #这一步涉及纹理映射的处理，以确保生成的纹理正确映射到 3D 模型的 UV 坐标上




        # #添加材质预测图进行UV映射和纹理更新!!!!!!!!!!!!!
        img_name = sample + f'_{view_idx}.png'
        material = cv2.imread(os.path.join(inference_dir, img_name), 0)

        # print("material.shape:",material.shape)



        init_material_, project_mask_image, exist_material, init_material_stack = backproject_from_image(
            mesh, faces, new_verts_uvs, cameras,
            material, generate_mask_image, generate_mask_image, init_material, exist_material, init_material_stack,
            image_size * 12, uv_size, 1,
            DEVICE
        )

        # pdb.set_trace()

        # update the mesh 更新 mesh 对象的纹理信息，包括颜色纹理（textures）、UV 坐标等。mesh 对象现在包含了新生成的纹理信息


        #保存时将texturing换为material

        # mesh.textures = TexturesUV(
        #     maps=transforms.ToTensor()(init_material)[None, ...].permute(0, 2, 3, 1).to(DEVICE),
        #     faces_uvs=faces.textures_idx[None, ...],
        #     verts_uvs=new_verts_uvs[None, ...]
        # )

        # 1.2.3. re: render
        # NOTE only the rendered image is needed - masks should be re-used
        # (
        #     view_score,
        #     renderer, cameras, fragments,
        #     init_image, *_,
        # ) = render_one_view_and_build_masks(B[view_idx],
        #     view_idx,  # => actual view idx and the sequence idx
        #     pre_similarity_texture_cache, exist_material,
        #     mesh, faces, new_verts_uvs,
        #     args.image_size, args.fragment_k,
        #     DEVICE, smooth_mask=args.smooth_mask, view_threshold=args.view_threshold
        # )

        # 1.3. update blurry region
        # only when: 1) use update flag; 2) there are contents to update; 3) there are enough contexts.
        """
        首先，检查是否允许更新 (not args.no_update)。
        检查是否有需要更新的内容 (update_mask_tensor.sum() > 0)。
        检查更新区域的像素数量是否占总区域的5%以上 (update_mask_tensor.sum() / (all_mask_tensor.sum()) > 0.05)。
        如果上述条件都满足，进入生成纹理的步骤
        
        """

        # 1.4. save generated assets
        # save backprojected OBJ file保存反投影的 OBJ 文件，包括了模型的顶点、UV坐标、纹理信息等


        # save_backproject_obj(
        #     mesh_dir, "{}.obj".format(view_idx),
        #     mesh_scale * mesh.verts_packed() + mesh_center if args.use_unnormalized else mesh.verts_packed(),
        #     faces.verts_idx, new_verts_uvs, faces.textures_idx, init_texture,
        #     DEVICE
        # )






        # save the intermediate view 保存中间渲染视图，即模型在生成过程中的中间结果，保存为图像文件



        # save texture mask将纹理蒙版（texture mask）保存为图像文件，以便进一步的分析和可视化



    print("=> total generate time: {} s".format(time.time() - start_time))

    # visualize viewpoints


    os.makedirs(UV_dir, exist_ok=True)
    np.save(os.path.join(UV_dir, 'UV_stack.npy'), init_material_stack)
    uv_stack = init_material_stack.copy()
    uv_stack = uv_stack[:, :, 1:]

    # weighted
    tmp = uv_stack[:, :, :5]
    uv_stack_new = np.concatenate((uv_stack, tmp), axis=2)

    uv_w, uv_h, y = uv_stack_new.shape
    flatten = uv_stack_new.reshape(-1, y).astype(np.int64)
    s, _ = flatten.shape

    new = np.zeros((s,)).astype(np.int64)
    for i in range(s):
        value = flatten[i]

        mask = np.isin(value, label_range, invert=True)
        value[mask] = 0
        count = np.bincount(value)

        if len(count) == 1:
            new[i] = 0
        else:

            new[i] = np.argmax(count[1:]) + 1

    result = new.reshape((uv_w, uv_h))

    result_scale = resize_without_interpolation(result, (1024, 1024))
    cv2.imwrite(os.path.join(UV_dir, 'predict_UV.png'), result_scale)
    print('save result!')

    result_rgb = to_rgb(result_scale, palette)

    # # region
    # predict_refine = solve(result_scale)
    # refine_rgb = to_rgb(predict_refine, palette)

    cv2.imwrite(os.path.join(UV_dir, 'result_palette.png'), result_rgb)
    # cv2.imwrite(os.path.join(uv_dir, sample, 'gt_palette.png'), gt_rgb)
    # cv2.imwrite(os.path.join(uv_dir, sample, 'refine_palette.png'), refine_rgb)
    # cv2.imwrite(os.path.join(uv_dir, sample, 'region_UV.png'), predict_refine)
    print('save rgb!')

    ORM_dir = UV_dir.replace('UV', 'ORM')
    os.makedirs(os.path.join(ORM_dir, sample), exist_ok=True)

    h, w = result_scale.shape
    ORM = np.zeros((h, w, 3))
    # pdb.set_trace()

    f = open("/path-to-MaterialSeg3D/Text2Tex/camera_angle/all_14.pkl", 'rb')
    # pdb.set_trace()
    mapping_dict = pickle.load(f)
    # pdb.set_trace()
    for mapping in mapping_dict:
        number = mapping['number']
        R, M = mapping['coordinates']
        if number == 4:
            ORM[result_scale == number] = [120, 20, 0]

        else:
            ORM[result_scale == number] = [255, round(R), round(M)]


    cv2.imwrite(os.path.join(ORM_dir, 'ORM.png'), ORM)

        
    
